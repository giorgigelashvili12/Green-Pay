# 1. What is Machine Learning? (ML)
ML is fundamentally just a science, in which, developers focus on teaching machines or computers to perform certain tasks without being given specific information. I mean, we want our machines to learn how to do something themselves without us having to explain it to them. Before doing this, looking at how human brain works and then design virtual brains that work in a similar manner can be a great example. Now you might think that ML and AI is the same since they can do stuff without guiding, but no, they're not. AI is a broad field and every system that can learn and solve problems can be considered as an AI, but ML is one specific approach. in ML the AI doesn't recieve any instructions. It isn't static and it doesn't follow clear and strict steps to solve problems. It's dynamic.

In ML we have different approaches, two main approaches are *supervised learning* and *unsupervised learning*.
## Supervised Learning
Let's take a model, a set of inputs and also the corresponding outputs,  which are desired results. In this way the model learns to match certain inputs to certain outputs and it adjusts its structure. It learns to make connections between what was put in and what the desired output is. When well trained enough, we can use the model to make predictions for inputs that we don't know the results are for.
## Unsupervised Learning
In unsupervised learning, we don't give our model the desired results while training, not because we don't want to but because we don't know them. This approach is more like a kind of pattern recognition. We give the model a set of input data and it then has to look for patterns in it. Once trained, we can put new data and our model will need to make decisions. Since it doesn't get information it has to work with similar patterns in the data.
## Reinforcement Learning
There's a third type of ML along with a 4oth one. Here, some model with a random structure is creared, then we observe what it does and encourage it, when we like what it does. Otherwise we can also give negative feedback. The more our model does what we want it to do, the more we reinforce it and the more encouragment it gets. In this way our model can learn what is right and what is wrong. You can imagine it a little bit like natural selection and survival of the fittest.
## Deep Learning
Deep learning is just one area of machine learning, namely the one which works with natural networks. Neural networks are a very comprehensive and a complex topic.
## Why Python?
Python is widely used for ML, since it provides a lot of frameworks and tools then any other languages. There are other languages like R (rust), MATLAB and LISP but overrall, python is recommended.

# 2. Setting Up - Modules
## NumPy
NumPy module allows us to work with vectors, matrices and multi-dimensional arrays, it is crucial for linear algebra and numerical analysis.
## Matplotlib
This library is responsible for plotting graphs and visualizing data, offering numerous types of plotting, styles and graphs.
## Pandas
Pandas offers a powerful data structure. This library allows us to efficiently work with our huge amounts of interrelated data. We can merge, reshape, filter and query our data. 
## scikit-learn
This library is porbably the most important Python library for traditional ML, it features classification, regression and clustering algorithms. It also wllows us to work with support vector machines.
## Tensorflow
This is one of the most popular ML frameworks, out there developed by Google. It's a whole ecosystem for developing modern deep learning models. This means that it is mainly used for the development and training of models.
```bash
pip install numpy
pip install matplotlib
pip install pandas
pip install scikit-learn
pip install tensorflow
```
# 3. Linear Regression
One of the most basic and easiest ML algorithm is linear regression. It will be the first one that we are going to look at and it is a supervised learning algorithmm meaning we need both inputs and outputs to train a model.

![Mathematical Linear Regression Example](./images/linear-math.png)

In the figure above, there are a lot of different points, which all have an `x-value` and `y-value`. The `x-value` is called the feature, whereas the `y-value` is the label. The label is the result for our feature. Our *linear rehression model* is represented by the blue line that goes straight through our data. It is placed so that it is as close as possible to all points at the same time, so we "trained" the line to fit the existing points or the existing data.
The idea is now to take a new x-value without knowing the corresponding y-value. We then look at the line and find the resulting y-value there, which the model predicts for us. Since this line is quite generalized, we will relatively inaccurate result.
## Loading Data
Before getting started, we need data we will work with (Dataset from UCI)
[UCI Dataset](https://archive.ics.uci.edu/dataset/320/student+performance)
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test+split
```
Our first action is to load the data from the CSV file into a Pandas Dataframe:
```python
data = pd.read_csv('student=mat.csv', sep=';')
```
it is important that we change our seperator to semicolon since the default seperator for CSV files in a comma and our file is seperated by semicolons. in the next step, we think about features that are relevant for us and what exactly we want to predict. A description of all features can be found on the previosuly mentined website. In this example, we will limit ourselves to the following columns: Age, Sex, Studytime, Absences, G1, G2, G3
```python
data = data[['age', 'sex', 'studytime', 'absences', 'G1', 'G2', 'G3']]
```
The columns *G1, G2, G3* are the three grades the students get. Our goal is to predict the third and final grade by looking at other values like first grade, age, sex and so on. We only select these columns from our DataFrame, out of the 33 possible. But we have a problem here, sex feature is not numeric, but stored as F or M. But for us to work with it and register it in the coordinate system, we have to convert it into numbers:
```python
data['sex'] = data['sex'].map({ 'F':0, 'M':1 })
```
as you can see, this id done by using the *map* function. Here we map a dictionary to our feature. Each F becomes a zero and every M becomes a one. Finally, we define the column of the desired label as a variable to make it easier to work with.
```python
prediction = 'G3'
```
## Preparing Data
Our data is now fully loaded and selected. However in order to use it as training and testing data for our model, we have to reformat them. The sklearn models do not accept Pandas data frames, but only NumPy arrays. That's why we turn our features into an x-array and our label into a y-aaray:
```python
X = np.array(data.drop{prediction},1)
Y = np.array(data[prediction])
```
This method now fully converts the selected columns into an array. The drop function returns the data frame without the specified column. Our X array now contains all of our columns, except for the final grade. The final grade is in the Y array.
In order to train and test our modelm we have to split our available data. The first part will be used to get the hyperplane, to fit our data as well as possible. The second part then checks the accuracy of the prediction, with previously unknown data:
```python
X_train, X_test, X_train, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)
```
With the function `train_test_split` we divide our X and Y arrays into four arrays. The order must be exactly as shown here. The `test_size` parameter specifies what percentage of records to use for testing, in this case, it is 10%. This is also a good and recommended value. This is done to test how accurate it is with data that our model has never seen before.
## Training and testing
First we define our model
```python
model = LinearRegression()
model.fit(X_train, Y_train)
```
By using the constructor of the `LinearRegression` class, we create our model. We then use the `fit` function and pass our training data. Now our model is already trained. It has now adjusted its hyperplane so that it fits all of our values. In order to test how well our model performs, we can use the `score` method and pass our testing data;
```python
accuracy = model.score(X_test, Y_test)
print(accuracy)
```
Since the splitting of training and test data is always random, we will have slightly different results on each run. An average result could look like this: **0.9130676521162756**, but actually, 91 percent is a pretty high and good accuracy. Now that we know that our model is reliable, we can enter new data and predict the final grade:
```python
X_new = np.array([[18, 1, 3, 40, 15, 16]])
Y_new = model.predict(X_new)
print(Y_new)
```
Here, NumPy array is defined with values for our features in the right order. Then we use the *predict* method to calcukate the likely final grade for our inputs. In this case the final grade would be probably 17.
## Visualizing Correlations
Since we are dealing iwth high dimensions here,we can't draw a graph of our model. This is only possible in two or three dimensions. However what we can visualize are relationships between individual features.
```python
plt.scatter(data['studytime'], data['G3'])
plt.title('Correlation')
plt.xlabel('Study Time')
plt.ylabel('Final Grade')
plt.show()
```
Here we draw a scatter plot with the function scatter, which shows the relationship between the learning time and the final grade.

![graph](./images/correlation.png)

In this case we can see that the relationship is not really strong, since the data is very diverse and you cannot see a clear pattern
```python
plt.scatter(data[ 'G2' ], data[ 'G3' ])
plt.title("Correlation")
plt.xlabel("Second Grade")
plt.ylabel("Final Grade")
plt.show()
```
But herem we look at the correlation between the second grade and the final gradem we see a much stronger correlation:

![graph1](./images/correlation1.png)

Here we can see that the students with good second grades are very lkely to end up with a good final grade as well. You can play around with diferent columns of this data set.
# 4. Classification
With regression we now predicted specific output-values for certain given input-values. Sometimes we are not trying to predict outputs but to categorize or classify our elements. For this we use classification algorithms.

![classification](./images/classification.png)

In given figure above we can see that one specific kind of classification algorithm, namely the *K-Nearest-Neighbor* classifier. Here we already have a decent amount of classified elements. We then add a new one (stars) and try to predict its class by looking at its nearest neighbors.
## Classification Algorithms
These are various different classification algorithms and they are often used for predicting medical data or other real life use-cases. For example, by providing a large amount of tumor samples, we can classify if a tumor is benign or malignant with a pretty high certainty.
## K-Nearest-Neighbors
As already mentioned by using the K-Nearest-Neighbors classifier, we assign the class of the new object, based on its nearest neighbors. The *k* specified the amount of neighbors to look at, e.g. we could say that we only want to look at the one neighbor who is nearest but we could also say that we want to factor in 100 neighbors. K shouldn't be a multiple of the number of classes since it might cause conflicts when we have an equal amount of elements from one class as from the other
## Naive-Bayes
The *Naive-bayes* algorithm might be a bit confusing when you encounter it the first name, howveer we are only going to discuss the basics and focus more on the implementation in Python later on.

![Table](./images/n-bytes.png)

Imagine that we have a table like the one given above, we have four input values and one label or output. The two classes are *yes* and *no* which indicate if we are going to play outside or not. But what *Naive bayes* do is to write down all the propabilities for the individual scenarios, so we should start by writing the general propability of playing and not playing. In this case, we only play three out of eight times and thus our probability of playing will be 3/8 and the probability of not playing will be 5/8. Out of the five times we had a high humidity we only played once, whereas out of the three times it was normal, we played twice, so our probability for playing when we have a high humidity is 1/5, and for playing when we have a medium humidity is 2/3. We go on like that and note all the probabilities we have in our table. Then get the classification for a new entry, we multply the probabilities together and end up with a prediction.
## Logistic Regression
Another popular classification algorithm is called `logistic regression`, even though the name says `regression` this is actually a classification algorithm. It looks at probabilities and determines how likely it is that a certain event happens, given the input data. This is done by plotting something similar to a logistic growth curve and splitting the data into two.

![Logistic Regression](./images/logistirc-regression.png)

Since we are not using a line we are also preventing mistakes caused by outliers.
## Decision Trees
With *decision tree* classifiers, we construct a decision tree out of training data and use it to predict the classes of new elements.

![Decision Tree](./images/decision-tree.png)

This classification algorithm requires very little data preparation and it is also very easy to understand and visualize. on the other hand, it is very easy to be overfitting the model. Here the model is very closely matched to the traning data and has worse chances to make a correct prediction on new data.
## Random Forest
The last classification algorithm of this chapter is the random forest classifier. It is based on decision trees. What it does is creating a forest of multiple decision trees. To classify a new object, all the various trees determine a class and the most frequent result gets chosen. This makes the result more accurate  and also prevents overfitting. It is also more suited to handle data sets with higher dimensions. On the other hand, since the generation of the forest is random, you gave very little control over your model.
## Loading Data
IN code example, we will get our data directly from the sklearn module:
```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer

data = load_breast_cancer()

print(data.feature_names)
print(data.target_names)
```
We load data with given function and get the names of the features and targets. Our features are all parameters that should help to determine the label or the taret. For the targets, we have two options in this dataset: malignant and benign

## Preparing Data
Once again, we convert our data back into NumPy arrays and split them into training and test data
```python
X = np.array(data.data)
Y = np.array(data.target)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)
```
The data attribute refers to our features and the target attribute points to the classes or labels. Then, we again choose a test size of ten percent.
## Training and testing
We first start by defining our `K-Nearest-Neighbors` classifier and then training it:
```python 
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, Y_train)
```
In this example the `n_neighbors` parameter specifies how many neighbor points we want to consider. In this case, we take five, then we test our model again for its accuracy:
```python 
accuracy = knn.score(X_test, Y_test)
print(accuracy)
```
After this, we get a decent accuracy for such complex task, such as `0.9649122807017544`
## Best Algorithm 
Now let's put all the classification algorithms that we've discussed up until now to use and see which one performs best.
```python 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bytes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
```
As we import all the models, we now can create five different classifiers to train and test them with the exact same data:
```python 
clf1 = KNeighborsClassifier(n_neighbors=5)
clf2 = GaussianNB()
clf3 = LogisticRegression()
clf4 = DecisionTreeClassifier()
clf5 = RandomForestClassifier()

clf1.fit(X_train, Y_train)
clf2.fit(X_train, Y_train)
clf3.fit(X_train, Y_train)
clf4.fit(X_train, Y_train)
clf5.fit(X_train, Y_train)

print(clf1.score(X_test, Y_test))
print(clf2.score(X_test, Y_test))
print(clf3.score(X_test, Y_test))
print(clf4.score(X_test, Y_test))
print(clf5.score(X_test, Y_test))
```
When running the program for the first time we can't really say which algorithm is the best, but everytime we run the script, we will see different results, for this specific data set.
## Predicting labels
We can again make predictions for new, unknown data. The chance of success in the classification is even very high. We need to pass an array of input values and use the `predict function`
```python 
X_new = np.array([[...]])
Y_new = clf.predict(X_new)
```
Unfortunately, visualizing data is impossible here since we have 30 features and cannot draw a 30-dimensinal coordinate system.
# 5. Vector Machines 
Topic after topic, mathematically things get bit more demanding. *Support Vector Machines* are very poweful, very effecient machine learning algorithms and they can even achieve much better results than neural network in some areas. We are again dealing with classification here but methodology is quite different. What we are looking for is a *hyperplane* that distinctly classifies our data points and has the maximum margin to all of our points. We want our model to be as generalized as possible.

![Feature one: 13 circle models, Feature 2: 14 triangle models (Vector)](./images/vector-machines.png)

In the graph, the model is very general and the line is the optimal function to seperate our data. We cam use an endless amount of lines to seperate the two classes but we don't want to overfit our model so that it only works for the data we already have. We also want it to work for unknown data.

![Better Vector Machine Model](./images/better-vector.png)

This model also seperates the data we already have perfectly, but we've got a new red point data here. When we look at it, it is obvious that this point belongs to the orange triangles, but however, our model classifies it as a blue circle because it is overfitting our current data. To find our perfect line we are using so-called `support vectors` which are parallel lines.

![Support Vectors](./images/vector-support.png)